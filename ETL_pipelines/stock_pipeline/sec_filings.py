# Importing external libraries:
import pandas as pd
import yfinance as yf
import requests
import bonobo
from bs4 import BeautifulSoup
import sqlite3
import time

# Importing the Base Pipeline API Object:
from ETL_pipelines.base_pipeline import Pipeline

class EDGARFilingsPipeline(Pipeline):
    """An object that contains all the logic and methods
    necessary to construct a ETL pipeline for extracting
    and ingesting SEC EDGAR Filings to a database.

    It inherits from the Pipeline API object with allows the extract,
    transform and load methods to be overwritten but the graph creation
    and pipeline execution methods to be inherited.

    Example:
        test_pipeline = EDGARFilingsPipeline("test.sqlite", "10-K")
        test_pipeline.ticker_lst = ["TSLA", "XOM"]
        test_pipeline.execute_pipeline()
 
     Arguments:
        dbpath (str): The relative or absoloute database URL pointing to
            the database where stock price data should be written.

        filings_type (str): The string that indicates the type of SEC
            EDGAR filings that are extracted by the pipeline. This string
            is passed into the url string used to make the request.
    """
    def __init__(self, dbpath, filings_type):
        
        # Declaring instance params:
        self.ticker_lst = []
        self.base_sec_url = "https://www.sec.gov"
        self.dbpath = dbpath
        self.filings_type = filings_type
        
    def extract_all_filings(self):
        """Method performs the raw data extraction from the "EDGAR Search Results"
        page. 
        
        This is done by making an HTTP GET request to the SEC EDGAR url that
        contains all of the relevant search params such as: 
        
        - CIK
        - Filing Type
        - Maximum # of Results (Default 100)

        This generator searches the extracted page contents for an html table containing
        all relevant filing data and converts this table into a dataframe. This dataframe 
        is then compared to existing filings data in the database and all already existing
        filings rows are dropped, making the dataframe contain only unique entries. The 'Format'
        column is transformed by converting the href link from the "Documents" button into a 
        full url to each filing's "Filing Detail" page.   

        The dataframe generated by the method is formatted as follows: 

        +----------------+-------+------+-----------+-----------+
        |File/Film Number|Filings|Format|Description|Filing Date|
        +----------------+-------+------+-----------+-----------+
        | String (index) |  str  |  str |    str    |    str    |
        +----------------+-------+------+-----------+-----------+
        
        Yields: 
        
            tuple: A tuple containing the ticker symbol string and the dataframe 
                of filings data from the 'EDGAR Search Results' page. (str, dataframe)

        """
        con = sqlite3.connect(self.dbpath)
        
        # Performing a query to the local database for the CIK:
        SPY_df = pd.read_sql_query("SELECT * FROM SPY_components", con)
                
        # Iterating through the ticker list to perform EDGAR query for each:
        for ticker in self.ticker_lst:
            
            # Searching existing database listings for CIK data:
            ticker_row = SPY_df.loc[SPY_df["Symbol"] == ticker]
            ticker_cik_arr = ticker_row['CIK'].array
            
            # If ticker CIK cannot be found internally, performing external request:
            if len(ticker_cik_arr) == 0:
                raise ValueError(f"CIK Not Found. {ticker} Does not appear to be in the database.")
            
            # Ticker CIK has been found. Continue logic:
            else:
                ticker_cik = ticker_cik_arr[0]
                
                # Performing Request to SEC EDGAR:
                edgar_result_url = self._build_edgar_url(cik=ticker_cik, filings_type=self.filings_type)
                edgar_result_response = requests.get(edgar_result_url)
                
                # Adding conditonal statements to catch response error:
                if edgar_result_response.status_code != 200:
                    raise ValueError(f"Request to EDGAR Search Results Page Failed with Status Code {edgar_result_response.status_code}")                
                else:
                    # Converting the response content to BeautifulSoup and parsing:
                    edgar_soup = BeautifulSoup(edgar_result_response.text, "html.parser")
                    filings_table = edgar_soup.find("table", {"class":"tableFile2"})
                    
                         # Building list of urls for each filing:
                    filings_urls = [
                        f"{self.base_sec_url}{href['href']}" for href in filings_table.find_all(
                        "a", {"id":"documentsbutton"}, href=True)]
                    
                    # Constructing a dataframe out of html content:
                    filings_df_lst = pd.read_html(str(filings_table))
                    filings_df = pd.DataFrame(filings_df_lst[0])
                    
                    # Adding the urls to each individual filings to df: 
                    filings_df["Format"] = filings_urls
                    filings_df.set_index("File/Film Number", inplace=True)
                    # Performing search of internal database for existing filings:
                    unique_filings_df = self._build_unique_filings(filings_df, ticker)

                    yield (ticker, unique_filings_df)

    def transform_add_filing_content(self, *args):
        """Method ingests a dataframe containing SEC EDGAR Filings data from the 
        "extract_all_filings" method and performs transformations on it that aggregates
        all relevant information in a way that makes the df ready to be ingested into the 
        dataframe.

        The method ingests the filings_df and performs the following transformations:

        - Renaming Columns to remove spaces to underscores.
        - Creating a new "Content" column by applying the _extract_individual_filing method
            to the "Format" column.

        The additional column "Content" is created by passing the url in the "Format" column into
        the _extract_individual_filings() method. The method takes the url and extracts the actual
        html content of the filings itself.    

        The dataframe built is as follows:

        +----------------+-------+-----------+-----------+-----------+-------+
        |File/Film Number|Filings|Filings_Url|Description|Filing_Date|Content|
        +----------------+-------+-----------+-----------+-----------+-------+
        | String (index) |  str  |     str   |    str    |    str    |  str  |
        +----------------+-------+-----------+-----------+-----------+-------+
        
        Arguments: 
            args (tuple): The only relevant argument passed into the method
                is the filings dataframe for specific ticker and the corresponding ticker symbol:
                (ticker, Dataframe).
        
        Yields:
            tuple: A tuple containing the ticker string and the dataframe containing all content.
            (ticker, dataframe)

        """
        # Unpacking tuple:
        ticker, filings_df = args[0], args[1]

        # Performing extraction of individual filings for each row in the dataframe:
        filings_df["Content"] = filings_df["Format"].map(lambda url: self._extract_individual_filing(url))

        # Formatting the dataframe to be written to the database:
        filings_df.rename(columns={"Format":"Filings_Url", "Filing Date":"Filing_Date"}, inplace=True)

        yield (ticker, filings_df)

    def load(self, *args):
        """Method writes the complete filings dataframe into
        the sqlite database.

        Arguments:
            args (tuple): The arguments passed into the load method by the transform method
                containing the dataframe and its associated ticker symbol. 

        """     
        # Unpacking arguments:
        ticker, filings_df = args[0], args[1]
        
        # Creating SQL Connection:
        con = sqlite3.connect(self.dbpath)

        # Writing the dataframe to the database:
        filings_df.to_sql(f"{ticker}_sec_filings", con, if_exists="append")

    def build_ticker_lst(self, filepath):
        """The method that opens the file containing ticker
        symbols and reads each symbol into the ticker_lst parameter.

        The method assumes ticker symbols are stored in a csv or txt file.

        Arguments:
            file_path (str): The absoloute or relative path to the file containing
                the ticker symbols.
        """
        self._ticker_filepath = filepath
        
        # Opening and exracting the information from the 
        ticker_file = open(self._ticker_filepath, 'rt')
        file_contents = ticker_file.read()
        
        # Seperating string into single list elements: 
        split_ticker_str = file_contents.split("\n")
        
        # Assigning split ticker list to the main param:
        self.ticker_lst = split_ticker_str
        
        ticker_file.close()
        
        # Converting the ticker list to a set and back to extract only unique elements:
        self.ticker_lst = list(set(self.ticker_lst))
        
    def _build_edgar_url(self, cik="", filings_type="", prior_to="", ownership="", no_of_entries=100):
        """ This internal method is used to contstruct a url to an 'EDGAR Search Results' page containing
        results that are refined by the various parameters.
        
        This is the url that is used via the extract_all_filings method to extract the inital search results page.

        Arguments:
            cik (str): The CIK identification number that is used to query a specific company.

            filings_type (str): The type of filings that are queries eg: "10-K", "10-Q".

            prior_to (str): The date before which all filing results are returned.

            ownership (str): If security ownership is returned in the results df or not.

            no_of_entries (int): The number of filings that are returned. Maximum value is 
            100 entries.
        
        Returns:
            str: The url to the "EDGAR Search Results" page
            
        """
        # Constructing the EDGAR query based on the search params:
        edgar_search_url = f"{self.base_sec_url}/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={filings_type}&dateb={prior_to}&owner={ownership}&count={str(no_of_entries)}"
        return edgar_search_url
    
    def _build_unique_filings(self, df, ticker):
        """The method that compares a dataframe of filing information extracted from the
        EDGAR page to existing filing information in the database and returns only filings 
        that do not exist in the database. 

        The method does this by the following logic: 
        
        - Make SQL Request for table containing filing data for specific ticker 
        - Parsing SQL table looking for entries that are also present in the input dataframe. 
        - Removing entries in the input df that are already present in the database.

        The method, instead of actually comparing dataframes for differences, compares the indexes
        of both dataframes. It extracts elements found in the input index not found in the database 
        table index and then slices the input dataframe via the new unique index.

        Arguments:
            df (pd.DataFrame): The input dataframe that is sliced for only unique elements.

            ticker (str): The ticker string of the assocaited dataframe 'df' that is used to build the
                database table query.
                
        """
        # Declaring localized connection:
        con = sqlite3.connect(self.dbpath)

        # Querying database for data table:
        tbl_name = f"{ticker}_sec_filings"
        tbl_query = f"SELECT * FROM {tbl_name}"
        
        # If no table exists, return the database_tbl param as none:
        try:
            database_tbl = pd.read_sql_query(tbl_query, con, index_col="File/Film Number")
        except:
            database_tbl = None

        # Conditional determining if dataframes need to be compared:
        if database_tbl is None:
            return df
        
        else:  
            # Comparing the two dataframes for unique elements:
            database_tbl_index = database_tbl.index
            df_index = df.index

            # Extracting the index values for rows that are not already in the database and building df: 
            unique_filing_rows = list(set(database_tbl_index) - set(df_index))
            df = df.loc[unique_filing_rows]
            
            return df
                             
    def _extract_individual_filing(self, url):
        """Method ingests a url to a SEC EDGAR 'Filing Detail' page and 
        performs an HTTP GET request to the first element in the tableFile
        table which contains the filing document content in HTML form.

        This is the method that is used to build a dataframe column of
        full filing document content by mapping it to another dataframe
        column containing "Filing Detail" urls.

        Arguments:
            url (str): The url to a SEC EDGAR Filing Detail page.

        Returns:
            str: A large string containing all the HTML content of a filing
                document. 
        """
        # Performing request to the Filing Detail Page:
        response = requests.get(url)
        time.sleep(5)

        # Converting the html content to the soup object and parsing:
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            filing_detail_tbl = soup.find("table", {"class":"tableFile"})

            # Try-Catch block meant to catch break in Filing Detail page structure:
            try:
                # Searching for first href in the Filing Detail Table:
                document_href = filing_detail_tbl.find_all("a", href=True)[0]["href"]
                document_href = document_href.replace("/ix?doc=", "") # Dropping XBRL label for only HTML document
                
                # Adding the SEC url to the href to build url to actual filing content:
                document_url = f"{self.base_sec_url}{document_href}"

                # Performing the Request to the document_url for filing content:
                filing_content = requests.get(document_url).text

                return filing_content

            except:
                return None

        else:
            raise ValueError(f"Response Status Code for {url} is {response.status_code}")
    
    def build_graph(self, **options):
        """The method that is used to construct a Bonobo ETL pipeline
        DAG that schedules the following ETL methods:

        - Extraction: extract_all_filings
        - Transformation: transform_add_filing_content
        - Loading: transform_add_filing_content

        Returns: 
            bonobo.Graph: The Bonobo Graph that is declared as an instance
                parameter and that will be executed by the self.execute_pipeline method.

        """

        # Building the Graph:
        self.graph = bonobo.Graph()    
        self.graph.add_chain(
            self.extract_all_filings,
            self.transform_add_filing_content,
            self.load)

        return self.graph
