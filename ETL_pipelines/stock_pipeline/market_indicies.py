# Importing data management libraries:
import pandas as pd
import yfinance as yf
import sqlite3

# Importing ETL libraries:
import bonobo

# Importing webscraping packages:
import requests 
from bs4 import BeautifulSoup 

# Importing the Base Pipeline API Object:
from ETL_pipelines.base_pipeline import Pipeline

class SPYCompositionPipeline(Pipeline):
    """A method that contains the logic necessary for constructing
    an ETL pipeline for a table containing the composition of the S&P 500 
    Stock Market Index from Wikipedia.

    It extends from the Base Parent object Pipeline and overwrites the 
    extract, transform and load methods.

    Reference:
        *  https://en.wikipedia.org/wiki/List_of_S%26P_500_companies
    
    Example:
        test = SPYCompositionPipeline('test.sqlite')
        test.execute_pipeline()


    Arguments:
        dbpath (str): The relative or absoloute database URL pointing to
            the database where stock price data should be written.

    """
    def __init__(self, dbpath):
        # Initalizing the parent method:
        super(SPYCompositionPipeline, self).__init__(dbpath)

        # Hard Coded URL for S&P 500 Index Contents:
        self.spy_comp_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

    # <-----------Bonobo ETL Methods----------->
    def extract(self):
        """Method that makes use of the requests python package
        to extract the table containing the S&P 500 Component Stocks 
        from Wikipedia.

        Yields: 

            requests.response: An HTTP response object that is the result of
                calling the request.get() method with the url to the wikipedia page. 
                This response object contains all of the content from the webpage.

        """
        # Perfroming HTTP request to extract a response object:
        response = requests.get(self.spy_comp_url)

        # Conditional logic to only generate values if valid response:
        if response.status_code == 200:
            yield response
        
        else:
            #TODO: Raise Error within else statement.
            yield None
        
    def transform(self, *args):
        """Method that parses the response object generated from the
        extract method into a dataframe.

        It makes uses of BeautifulSoup and pandas libraries to parse the 
        html contents of the HTTP response object and transform it into a
        dataframe containing all SPY components.

        Arguments:
            args (tuple): A length-1 tuple containing the response object
                generated from the extract method. 

        Yield:
            pandas.Dataframe: The dataframe that contains the formatted 
                SPY components extracted from response objects.

        """
        if args[0] is None:
            return

        # Unpacking args tuple:
        response = args[0]

        # Converting raw html content into bs4 object and parsing for table:
        soup = BeautifulSoup(response.text, "html.parser")
        table_content = soup.find("table", {"id":"constituents"}) # <table id='constituents'>

        # Converting the HTML content to pandas Dataframe:
        components_df_lst = pd.read_html(str(table_content))
        components_df = pd.DataFrame(components_df_lst[0])

        # Performing data formatting:
        components_df.drop(columns=["SEC filings"], inplace=True)
        components_df.set_index("Symbol")

        yield components_df

    def load(self, *args):
        """Method uses the pandas method to write the dataframe 
        generated by the transform method to a database.

        Arguments:
            args (tuple): A length-1 tuple containing the formatted
                dataframe of components generted by the transform method

        """
        component_df = args[0]

        # Creating a connection to the database:
        self._con = sqlite3.connect(self.dbpath)
        
        # Writing data to the database:
        component_df.to_sql("SPY_components", self._con, if_exists="replace", index=False)  
