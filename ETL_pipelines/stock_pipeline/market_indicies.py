# Importing data management libraries:
import pandas as pd
import yfinance as yf
import sqlite3

# Importing ETL libraries:
import bonobo

# Importing webscraping packages:
import requests 
from bs4 import BeautifulSoup 

# Importing the Base Pipeline API Object:
from ETL_pipelines.base_pipeline import Pipeline

class SPYCompositionPipeline(Pipeline):
    """A method that contains the logic necessary for constructing
    an ETL pipeline for a table containing the composition of the S&P 500 
    Stock Market Index from Wikipedia.

    It extends from the Base Parent object Pipeline and overwrites the 
    extract, transform and load methods.

    Reference:
        *  https://en.wikipedia.org/wiki/List_of_S%26P_500_companies
    
    Example:
        test = SPYCompositionPipeline('test.sqlite')
        test.execute_pipeline()


    Arguments:
        dbpath (str): The relative or absoloute database URL pointing to
            the database where stock price data should be written.

    """
    def __init__(self, dbpath):
        # Initalizing the parent method:
        super(SPYCompositionPipeline, self).__init__(dbpath)

        # Hard Coded URL for S&P 500 Index Contents:
        self.spy_comp_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

    # <-----------Bonobo ETL Methods----------->
    def extract(self):
        """Method that makes use of the requests python package
        to extract the table containing the S&P 500 Component Stocks 
        from Wikipedia.

        Yields: 

            requests.response: An HTTP response object that is the result of
                calling the request.get() method with the url to the wikipedia page. 
                This response object contains all of the content from the webpage.

        """
        # Perfroming HTTP request to extract a response object:
        response = requests.get(self.spy_comp_url)

        # Conditional logic to only generate values if valid response:
        if response.status_code == 200:
            yield response
        
        else:
            raise ValueError(f"Response Status Code not 200, it is {response.status_code}")
        
    def transform(self, *args):
        """Method that parses the response object generated from the
        extract method into a dataframe.

        It makes uses of BeautifulSoup and pandas libraries to parse the 
        html contents of the HTTP response object and transform it into a
        dataframe containing all SPY components.

        Arguments:
            args (tuple): A length-1 tuple containing the response object
                generated from the extract method. 

        Yield:
            pandas.Dataframe: The dataframe that contains the formatted 
                SPY components extracted from response objects.

        """
        # Unpacking args tuple:
        response = args[0]

        # Converting raw html content into bs4 object and parsing for table:
        soup = BeautifulSoup(response.text, "html.parser")
        table_content = soup.find("table", {"id":"constituents"}) # <table id='constituents'>

        # Converting the HTML content to pandas Dataframe:
        components_df_lst = pd.read_html(str(table_content))
        components_df = pd.DataFrame(components_df_lst[0])

        # Performing data formatting:
        components_df.drop(columns=["SEC filings"], inplace=True)
        components_df.set_index("Symbol")

        yield components_df

    def load(self, *args):
        """Method uses the pandas method to write the dataframe 
        generated by the transform method to a database.

        Arguments:
            args (tuple): A length-1 tuple containing the formatted
                dataframe of components generted by the transform method

        """
        component_df = args[0]

        # Creating a connection to the database:
        self._con = sqlite3.connect(self.dbpath)
        
        # Writing data to the database:
        component_df.to_sql("SPY_components", self._con, if_exists="replace", index=False)  

class DJIACompositionPipeline(Pipeline):
    """A method that contains the logic necessary for constructing
    an ETL pipeline for a table containing the composition of the Dow Jones 
    Industrial Average Stock Market Index from Wikipedia.

    It extends from the Base Parent object Pipeline and overwrites the 
    extract, transform and load methods.

    Reference:
        *  https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average
    
    Example:
        test = DJICompositionPipeline('test.sqlite')
        test.execute_pipeline()


    Arguments:
        dbpath (str): The relative or absoloute database URL pointing to
            the database where stock price data should be written.

    """
    def __init__(self, dbpath):
        # Initalizing the parent method:
        super(DJIACompositionPipeline, self).__init__(dbpath)

        # Hard Coded URL for S&P 500 Index Contents:
        self.djia_comp_url = "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"

    def extract(self):
        """Method that makes use of the requests python package
        to extract the table containing the Dow Jones Industrial Average 
        Component Stocks from Wikipedia.

        Yields: 
            requests.response: An HTTP response object that is the result of
                calling the request.get() method with the url to the wikipedia page. 
                This response object contains all of the content from the webpage.

        """
        # Performing HTTP requests:
        response = requests.get(self.djia_comp_url)
        if response.status_code == 200:
            yield response

        else:
            raise ValueError(f"Response Status Code not 200, it is {response.status_code}")
        
    def transform(self, *args):
        """Method that parses the response object from the
        extract method.

        It extracts the html content from the response object and through 
        BeautifulSoup and pandas converts the html table into a pandas 
        dataframe.

        Arguments:
            args (tuple): A length-1 tuple containing the response object
                generated from the extract method. 

        Yield:
            pandas.Dataframe: The dataframe that contains the formatted 
                DJIA components extracted from response objects.

        """
        print("HELLO")
        html_content = args[0].text

        # Internal Method for Formatting a Column in component_df:
        def _format_listings_symbol_str(ticker_str):
            """The dataframe for DJIA components contains 
            a column 'Symbol' which represents a ticker symbol 
            as:

            NYSE: PG or PG.

            The method takes a string as an input and drops the 
            'NYSE: ' elements of the string.

            Arguments:
                ticker_str (str): The string representing each ticker
                    symbol. Each cell of the Symbol column row. Can be
                    'NYSE: PG' or 'PG'

            Returns:
                str: The formatted string to replace the corresponding value
                   in the column.

            """
            return ticker_str.replace("NYSE:", "")

        # Converting the HTML content into a BeautifulSoup object and parsing:
        soup = BeautifulSoup(html_content, "html.parser")
        table_content = soup.find("table", {"id":"constituents"}) # <table id='constituents'>

        # Converting Soup table object to HTML to be converted to pandas Dataframe:
        component_df_lst = pd.read_html(str(table_content))
        component_df = pd.DataFrame(component_df_lst[0])                
        component_df["Symbol"] = component_df["Symbol"].apply(_format_listings_symbol_str)
            
        yield component_df

    def load(self, *args):
        """Method uses the pandas method to write the dataframe 
        generated by the transform method to a database.

        Arguments:
            args (tuple): A length-1 tuple containing the formatted
                dataframe of components generted by the transform method

        """
        component_df = args[0]

        # Creating a connection to the database:
        self._con = sqlite3.connect(self.dbpath)

        # Writing data to the database:
        component_df.to_sql("DJIA_components", self._con, if_exists="replace", index=False)  
