{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR Pipeline API:\n",
    "An ETL Pipleine built with Bonobo that ingests all of the SEC EDGAR Filings data for a specific stock.\n",
    "\n",
    "### Step 1: Create logic that constructs a url for the EDGAR filings based on input parameters:\n",
    "```python\n",
    "build_edgar_url(\"AAPL\", \"10-K\", max=100) -> \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=exclude&count=100&search_text=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries:\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ETL_pipelines.base_pipeline import Pipeline\n",
    "\n",
    "# Local SQLITE database for testing:\n",
    "test_dbpath = \"../ETL_pipelines/stock_pipeline/test.sqlite\"\n",
    "\n",
    "# Local ticker text file for testing:\n",
    "ticker_txt = \"../ETL_pipelines/stock_pipeline/example.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the main pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARFilingsPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    ADD DOCUMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, dbpath):\n",
    "        \n",
    "        # Declaring instance params:\n",
    "        self.ticker_lst = []\n",
    "        self.base_sec_url = \"https://www.sec.gov\"\n",
    "        self.dbpath = dbpath\n",
    "        \n",
    "    def extract_filings_page(self):\n",
    "        \"\"\"Method performs the \n",
    "        \"\"\"\n",
    "        con = sqlite3.connect(self.dbpath)\n",
    "        \n",
    "        # Performing a query to the local database for the CIK:\n",
    "        SPY_df = pd.read_sql_query(\"SELECT * FROM SPY_components\", con)\n",
    "                \n",
    "        # Iterating through the ticker list to perform EDGAR query for each:\n",
    "        for ticker in self.ticker_lst:\n",
    "            \n",
    "            # Searching existing database listings for CIK data:\n",
    "            ticker_row = SPY_df.loc[SPY_df[\"Symbol\"] == ticker]\n",
    "            ticker_cik_arr = ticker_row['CIK'].array\n",
    "            \n",
    "            # If ticker CIK cannot be found internally, performing external request:\n",
    "            if len(ticker_cik_arr) == 0:\n",
    "                pass # TODO: Add Error Catch.\n",
    "            \n",
    "            # Ticker CIK has been found. Continue logic:\n",
    "            else:\n",
    "                ticker_cik = ticker_cik_arr[0]\n",
    "                \n",
    "                # Performing Request to SEC EDGAR:\n",
    "                edgar_result_url = self._build_edgar_url(cik=ticker_cik, filings_type=\"10-K\")\n",
    "                edgar_result_response = requests.get(edgar_result_url)\n",
    "                \n",
    "                # Adding conditonal statements to catch response error:\n",
    "                if edgar_result_response.status_code != 200:\n",
    "                    pass # TODO: Add Error Catch\n",
    "                \n",
    "                else:\n",
    "                    # Converting the response content to BeautifulSoup and parsing:\n",
    "                    edgar_soup = BeautifulSoup(edgar_result_response.text, \"html.parser\")\n",
    "                    filings_table = edgar_soup.find(\"table\", {\"class\":\"tableFile2\"})\n",
    "                    \n",
    "                         # Building list of urls for each filing:\n",
    "                    filings_urls = [\n",
    "                        f\"{self.base_sec_url}{href['href']}\" for href in filings_table.find_all(\n",
    "                        \"a\", {\"id\":\"documentsbutton\"}, href=True)]\n",
    "                    \n",
    "                    # Constructing a dataframe out of html content:\n",
    "                    filings_df_lst = pd.read_html(str(filings_table))\n",
    "                    filings_df = pd.DataFrame(filings_df_lst[0])\n",
    "                    \n",
    "                    # Adding the urls to each individual filings to df: \n",
    "                    filings_df[\"Format\"] = filings_urls\n",
    "                    filings_df.set_index(\"Filing Date\", inplace=True)\n",
    "                    \n",
    "                    # Performing search of internal database for existing filings:\n",
    "                    unique_filings_df = self._build_unique_filings(filings_df, ticker, \"10-K\")\n",
    "                    \n",
    "                    # Passing filings dataframe to the next extraction method:\n",
    "                    if unique_filings_df is not None:\n",
    "                        yield (ticker, unique_filings_df)\n",
    "                        \n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "    def build_ticker_lst(self, filepath):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._ticker_filepath = filepath\n",
    "        \n",
    "        # Opening and exracting the information from the \n",
    "        ticker_file = open(self._ticker_filepath, 'rt')\n",
    "        file_contents = ticker_file.read()\n",
    "        \n",
    "        # Seperating string into single list elements: \n",
    "        split_ticker_str = file_contents.split(\"\\n\")\n",
    "        \n",
    "        # Assigning split ticker list to the main param:\n",
    "        self.ticker_lst = split_ticker_str\n",
    "        \n",
    "        ticker_file.close()\n",
    "        \n",
    "        # Converting the ticker list to a set and back to extract only unique elements:\n",
    "        self.ticker_lst = list(set(self.ticker_lst))\n",
    "        \n",
    "        \n",
    "    def _build_edgar_url(self, cik=\"\", filings_type=\"\", prior_to=\"\", ownership=\"\", no_of_entries=100):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Constructing the EDGAR query based on the search params:\n",
    "        edgar_search_url = f\"{self.base_sec_url}/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={filings_type}&dateb={prior_to}&owner={ownership}&count={str(no_of_entries)}\"\n",
    "        return edgar_search_url\n",
    "    \n",
    "    def _build_unique_filings(self, df, ticker, filing_type):\n",
    "        \"\"\"TODO: ADD Documentation.\n",
    "        \n",
    "        - Make SQL Request for table containing filing data for specific ticker \n",
    "        - Parsing SQL table looking for entries that are also present in the input dataframe. \n",
    "        - Removing entries in the input df that are already present in the database.\n",
    "        \"\"\"\n",
    "        # Querying database for data table:\n",
    "        tbl_name = f\"{ticker}_{filing_type}_filings\"\n",
    "        tbl_query = f\"SELECT * FROM {tbl_name}\"\n",
    "        \n",
    "        # Try-Catch to declare db_tbl as None if it does not exist:\n",
    "        try:\n",
    "            database_tbl = pd.read_sql_query(tbl_query, con)\n",
    "        except:\n",
    "            database_tbl = None\n",
    "            \n",
    "        # Conditional determining if dataframes need to be compared:\n",
    "        if database_tbl == None:\n",
    "            return None\n",
    "        \n",
    "        # TODO: Write database comparing methods:\n",
    "        else:\n",
    "            # Comparing the two dataframes for unique elements:\n",
    "            for index, row in df.iterrows():\n",
    "                print(row)\n",
    "\n",
    "        \n",
    "    \n",
    "# Declaring Example Test Pipeline:\n",
    "test_pipeline = EDGARFilingsPipeline(test_dbpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Object Builds the Correct EDGAR Search URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=&count=100\n"
     ]
    }
   ],
   "source": [
    "print(test_pipeline._build_edgar_url(cik= '0000320193', filings_type=\"10-K\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Object Constructs Ticker List from Text File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker list before build method: []\n",
      "Ticker list after build method: ['TSLA', 'SPY', 'ICLN', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ticker list before build method:\", test_pipeline.ticker_lst)\n",
    "test_pipeline.build_ticker_lst(ticker_txt)\n",
    "print(\"Ticker list after build method:\", test_pipeline.ticker_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Objects Performs Extraction of EDGAR Search Papers for each Ticker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object EDGARFilingsPipeline.extract_filing_pages at 0x7faf92085890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipeline.extract_filings_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
