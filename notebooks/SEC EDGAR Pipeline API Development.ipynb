{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR Pipeline API:\n",
    "An ETL Pipleine built with Bonobo that ingests all of the SEC EDGAR Filings data for a specific stock.\n",
    "\n",
    "### Step 1: Create logic that constructs a url for the EDGAR filings based on input parameters:\n",
    "```python\n",
    "build_edgar_url(\"AAPL\", \"10-K\", max=100) -> \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=exclude&count=100&search_text=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries:\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ETL_pipelines.base_pipeline import Pipeline\n",
    "\n",
    "# Local SQLITE database for testing:\n",
    "test_dbpath = \"../ETL_pipelines/stock_pipeline/test.sqlite\"\n",
    "\n",
    "# Local ticker text file for testing:\n",
    "ticker_txt = \"../ETL_pipelines/stock_pipeline/example.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the main pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDGARFilingsPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    ADD DOCUMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, dbpath):\n",
    "        \n",
    "        # Declaring instance params:\n",
    "        self.ticker_lst = []\n",
    "        self.base_sec_url = \"https://www.sec.gov\"\n",
    "        self.dbpath = dbpath\n",
    "        \n",
    "    def extract_filings_page(self):\n",
    "        \"\"\"Method performs the \n",
    "        \"\"\"\n",
    "        con = sqlite3.connect(self.dbpath)\n",
    "        \n",
    "        # Performing a query to the local database for the CIK:\n",
    "        SPY_df = pd.read_sql_query(\"SELECT * FROM SPY_components\", con)\n",
    "                \n",
    "        # Iterating through the ticker list to perform EDGAR query for each:\n",
    "        for ticker in self.ticker_lst:\n",
    "            \n",
    "            # Searching existing database listings for CIK data:\n",
    "            ticker_row = SPY_df.loc[SPY_df[\"Symbol\"] == ticker]\n",
    "            ticker_cik_arr = ticker_row['CIK'].array\n",
    "            \n",
    "            # If ticker CIK cannot be found internally, performing external request:\n",
    "            if len(ticker_cik_arr) == 0:\n",
    "                pass # TODO: Add Error Catch.\n",
    "            \n",
    "            # Ticker CIK has been found. Continue logic:\n",
    "            else:\n",
    "                ticker_cik = ticker_cik_arr[0]\n",
    "                \n",
    "                # Performing Request to SEC EDGAR:\n",
    "                edgar_result_url = self._build_edgar_url(cik=ticker_cik, filings_type=\"10-K\")\n",
    "                edgar_result_response = requests.get(edgar_result_url)\n",
    "                \n",
    "                # Adding conditonal statements to catch response error:\n",
    "                if edgar_result_response.status_code != 200:\n",
    "                    pass # TODO: Add Error Catch\n",
    "                \n",
    "                else:\n",
    "                    # Converting the response content to BeautifulSoup and parsing:\n",
    "                    edgar_soup = BeautifulSoup(edgar_result_response.text, \"html.parser\")\n",
    "                    filings_table = edgar_soup.find(\"table\", {\"class\":\"tableFile2\"})\n",
    "                    \n",
    "                         # Building list of urls for each filing:\n",
    "                    filings_urls = [\n",
    "                        f\"{self.base_sec_url}{href['href']}\" for href in filings_table.find_all(\n",
    "                        \"a\", {\"id\":\"documentsbutton\"}, href=True)]\n",
    "                    \n",
    "                    # Constructing a dataframe out of html content:\n",
    "                    filings_df_lst = pd.read_html(str(filings_table))\n",
    "                    filings_df = pd.DataFrame(filings_df_lst[0])\n",
    "                    \n",
    "                    # Adding the urls to each individual filings to df: \n",
    "                    filings_df[\"Format\"] = filings_urls\n",
    "                    filings_df.set_index(\"Filing Date\", inplace=True)\n",
    "                    \n",
    "                    # Performing search of internal database for existing filings:\n",
    "                    unique_filings_df = self._build_unique_filings(filings_df, ticker)\n",
    "                    \n",
    "                    yield (ticker, unique_filings_df)\n",
    "    \n",
    "    \n",
    "    def build_ticker_lst(self, filepath):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._ticker_filepath = filepath\n",
    "        \n",
    "        # Opening and exracting the information from the \n",
    "        ticker_file = open(self._ticker_filepath, 'rt')\n",
    "        file_contents = ticker_file.read()\n",
    "        \n",
    "        # Seperating string into single list elements: \n",
    "        split_ticker_str = file_contents.split(\"\\n\")\n",
    "        \n",
    "        # Assigning split ticker list to the main param:\n",
    "        self.ticker_lst = split_ticker_str\n",
    "        \n",
    "        ticker_file.close()\n",
    "        \n",
    "        # Converting the ticker list to a set and back to extract only unique elements:\n",
    "        self.ticker_lst = list(set(self.ticker_lst))\n",
    "        \n",
    "        \n",
    "    def _build_edgar_url(self, cik=\"\", filings_type=\"\", prior_to=\"\", ownership=\"\", no_of_entries=100):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Constructing the EDGAR query based on the search params:\n",
    "        edgar_search_url = f\"{self.base_sec_url}/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={filings_type}&dateb={prior_to}&owner={ownership}&count={str(no_of_entries)}\"\n",
    "        return edgar_search_url\n",
    "    \n",
    "    def _build_unique_filings(self, df, ticker):\n",
    "        \"\"\"TODO: ADD Documentation.\n",
    "        \n",
    "        - Make SQL Request for table containing filing data for specific ticker \n",
    "        - Parsing SQL table looking for entries that are also present in the input dataframe. \n",
    "        - Removing entries in the input df that are already present in the database.\n",
    "        \"\"\"\n",
    "        # Querying database for data table:\n",
    "        tbl_name = f\"{ticker}_filings\"\n",
    "        tbl_query = f\"SELECT * FROM {tbl_name}\"\n",
    "        \n",
    "        # Try-Catch to declare db_tbl as None if it does not exist:\n",
    "        try:\n",
    "            database_tbl = pd.read_sql_query(tbl_query, con)\n",
    "        except:\n",
    "            database_tbl = None\n",
    "            \n",
    "        # Conditional determining if dataframes need to be compared:\n",
    "        if database_tbl == None:\n",
    "            return df\n",
    "        \n",
    "        # TODO: Write database comparing methods:\n",
    "        else:\n",
    "            # Comparing the two dataframes for unique elements:\n",
    "            for index, row in df.iterrows():\n",
    "                print(row)\n",
    "                \n",
    "    def _extract_individual_filing(url):\n",
    "        \"\"\"Method ingests a url to the SEC EDGAR webpage that\n",
    "        lists an individual filing and returns the html content\n",
    "        of said filing.\n",
    "\n",
    "        This html content is extracted from the href of the first\n",
    "        element in the results table.\n",
    "\n",
    "        TODO: Fully articulate this documentation.\n",
    "        \"\"\"\n",
    "        # Performing request to the Filing Detail Page:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Converting the html content to the soup object and parsing:\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            filing_detail_tbl = soup.find(\"table\", {\"class\":\"tableFile\"})\n",
    "\n",
    "            # Try-Catch block meant to catch break in Filing Detail page structure:\n",
    "            try:\n",
    "                # Searching for first href in the Filing Detail Table:\n",
    "                document_href = filing_detail_tbl.find_all(\"a\", href=True)[0][\"href\"]\n",
    "                document_href = document_href.replace(\"/ix?doc=\", \"\") # Dropping XBRL label for only HTML document\n",
    "\n",
    "                # Adding the SEC url to the href to build url to actual filing content:\n",
    "                document_url = f\"https://www.sec.gov{document_href}\"\n",
    "\n",
    "                # Performing the Request to the document_url for filing content:\n",
    "                filing_content = requests.get(document_url).text\n",
    "\n",
    "                return filing_content\n",
    "\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Response Status Code for {url} is {response.status_code}\")\n",
    "\n",
    "\n",
    "    \n",
    "# Declaring Example Test Pipeline:\n",
    "test_pipeline = EDGARFilingsPipeline(test_dbpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Object Builds the Correct EDGAR Search URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=&owner=&count=100\n"
     ]
    }
   ],
   "source": [
    "print(test_pipeline._build_edgar_url(cik= '0000320193', filings_type=\"10-K\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Object Constructs Ticker List from Text File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker list before build method: []\n",
      "Ticker list after build method: ['SPY', 'XOM', 'TSLA', 'ICLN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ticker list before build method:\", test_pipeline.ticker_lst)\n",
    "test_pipeline.build_ticker_lst(ticker_txt)\n",
    "print(\"Ticker list after build method:\", test_pipeline.ticker_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that extracts the main document from a Filing Detail page link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _extract_individual_filing(url):\n",
    "    \"\"\"Method ingests a url to the SEC EDGAR webpage that\n",
    "    lists an individual filing and returns the html content\n",
    "    of said filing.\n",
    "        \n",
    "    This html content is extracted from the href of the first\n",
    "    element in the results table.\n",
    "        \n",
    "    TODO: Fully articulate this documentation.\n",
    "    \"\"\"\n",
    "    # Performing request to the Filing Detail Page:\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Converting the html content to the soup object and parsing:\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        filing_detail_tbl = soup.find(\"table\", {\"class\":\"tableFile\"})\n",
    "        \n",
    "        # Try-Catch block meant to catch break in Filing Detail page structure:\n",
    "        try:\n",
    "            # Searching for first href in the Filing Detail Table:\n",
    "            document_href = filing_detail_tbl.find_all(\"a\", href=True)[0][\"href\"]\n",
    "            document_href = document_href.replace(\"/ix?doc=\", \"\") # Dropping XBRL label for only HTML document\n",
    "            \n",
    "            # Adding the SEC url to the href to build url to actual filing content:\n",
    "            document_url = f\"https://www.sec.gov{document_href}\"\n",
    "           \n",
    "            # Performing the Request to the document_url for filing content:\n",
    "            filing_content = requests.get(document_url).text\n",
    "            \n",
    "            return filing_content\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(f\"Response Status Code for {url} is {response.status_code}\")\n",
    "\n",
    "# Example implementation of method: \n",
    "#print(_extract_individual_filing(\"https://www.sec.gov/Archives/edgar/data/34088/000003408820000016/0000034088-20-000016-index.htm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDGARFilingsPipeline Objects Performs Extraction of EDGAR Search Papers for each Ticker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object EDGARFilingsPipeline.extract_filings_page at 0x7ffab515d890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pipeline.extract_filings_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that compares two dataframes that have the same index and extracts the unique elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [one, three]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def build_unique_dataframe(df1, df2):\n",
    "    \"\"\"Method compares two dataframes with the same index and\n",
    "    drops the rows that are duplicates. It then returns a dataframe\n",
    "    containing only the unique elements.\n",
    "    \"\"\"\n",
    "    index_a = df1.index\n",
    "    index_b = df2.index\n",
    "    \n",
    "    unique_index = list((set(index_a) | set(index_b)) - (set(index_a) & set(index_b)))    \n",
    "    \n",
    "    df = df2.loc[unique_index]\n",
    "    print(df)\n",
    "    \n",
    "# Intialise data to Dicts of series. \n",
    "dict1 = {'one' : pd.Series([10, 20, 30, 40], \n",
    "                       index =['a', 'b', 'c', 'd']), \n",
    "      'two' : pd.Series([10, 20, 30, 40], \n",
    "                        index =['a', 'b', 'c', 'd'])} \n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "dict2 = {'one' : pd.Series([10, 20, 30, 40], \n",
    "                       index =['a', 'b', 'c', 'd']), \n",
    "      'three' : pd.Series([10, 20, 30, 40], \n",
    "                        index =['a', 'b', 'c', 'd'])}\n",
    "\n",
    "df1 = pd.DataFrame(dict1)\n",
    "df2 = pd.DataFrame(dict2)\n",
    "\n",
    "build_unique_dataframe(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
